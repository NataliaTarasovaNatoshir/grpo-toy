{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GRPO (Group Relative Policy Optimization) Training Loop\n",
        "\n",
        "This notebook demonstrates GRPO training on a simple arithmetic task. GRPO is a variant of PPO that uses group-based advantage estimation instead of value functions.\n",
        "\n",
        "**Key idea**: Generate multiple responses per prompt, compute advantages relative to the group mean, then optimize with PPO-style clipping."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3m7Qmji19tJ",
        "outputId": "8b4f3dda-b2ec-43a7-9a85-86b5df48a13d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Basic config - using small token limit since we're doing simple arithmetic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VHJ90L_k6ipO"
      },
      "outputs": [],
      "source": [
        "device=\"cuda\"\n",
        "max_new_tokens = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "I-E-Kdwt2Hlp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Setup\n",
        "\n",
        "Load two copies of the same model:\n",
        "- `model`: trainable policy we'll optimize\n",
        "- `ref_model`: frozen reference policy for KL regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TtJFIFnq2NKH"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model_id = \"HuggingFaceTB/SmolLM-135M-Instruct\"  # a small model that fits comfortably on T4\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "ref_model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "ref_model.to(device)\n",
        "ref_model.eval()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task: Simple Arithmetic\n",
        "\n",
        "Generate random arithmetic problems (addition/multiplication) with single digits. Simple but non-trivial for small models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wKd5U4mD2P1x"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "def get_batch(size=10):\n",
        "  batch = []\n",
        "  for _ in range(size):\n",
        "    a = random.randint(0, 9)\n",
        "    b = random.randint(0, 9)\n",
        "    op = random.choice([\"+\", \"*\"])\n",
        "    if op == \"+\":\n",
        "      target = a + b\n",
        "    else:\n",
        "      target = a * b\n",
        "    batch.append((f\"Solve {a}{op}{b}=\", str(target)))\n",
        "  return batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Group Generation\n",
        "\n",
        "Core GRPO component: generate multiple responses per prompt. The mask tracks which tokens are newly generated (for computing log probabilities)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "itItLkhR3siX"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def generate_group(model, tokenizer, prompt, group_size=4, temperature=0.7,\n",
        "                   max_new_tokens=3):\n",
        "  input = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "  output = model.generate(**input, do_sample=True,\n",
        "                          num_return_sequences=group_size,\n",
        "                          temperature=temperature,\n",
        "                          max_new_tokens=max_new_tokens)\n",
        "  mask = torch.zeros(output.size()).to(device)\n",
        "  mask[:, -max_new_tokens:] = 1\n",
        "  return output, mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reward and Evaluation\n",
        "\n",
        "Binary reward: 1.0 if the model's answer matches the target, 0.0 otherwise. Simple but effective for this task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QQuXRy_W9LOT"
      },
      "outputs": [],
      "source": [
        "def extract_answer(response):\n",
        "  if \"=\" not in response:\n",
        "    return None\n",
        "  return \"\".join(response.split(\"=\")[1:]).strip()\n",
        "\n",
        "def reward(response, target):\n",
        "  return 1.0 if extract_answer(response) == target else 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluation on a random batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def eval(model, batch):\n",
        "  rewards = []\n",
        "  responses = []\n",
        "  input = tokenizer([b[0] for b in batch], return_tensors=\"pt\").to(device)\n",
        "  output = model.generate(**input, max_new_tokens=max_new_tokens, do_sample=False)  # greedy generation\n",
        "  for i, o in enumerate(output):\n",
        "    response = tokenizer.decode(o, skip_special_tokens=True)\n",
        "    responses.append(response)\n",
        "    rewards.append(reward(response, batch[i][1]))\n",
        "\n",
        "  acc = sum(rewards) / len(rewards)\n",
        "  print(f\"Average accuracy = {acc}\\n\")\n",
        "  \n",
        "  # Show 10 random examples\n",
        "  sample_indices = random.sample(range(len(batch)), min(10, len(batch)))\n",
        "  for i in sample_indices:\n",
        "    is_correct = rewards[i] == 1.0\n",
        "    symbol = '✓' if is_correct else '✗'\n",
        "    print(f\"{symbol} {responses[i].strip()}. Target: {batch[i][1]}\"),\n",
        "    \n",
        "  \n",
        "  return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GRPO Loss Function\n",
        "\n",
        "The GRPO loss from Shao et al. 2024 (\"DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models\"):\n",
        "\n",
        "$$J_{GRPO}(\\theta) = \\frac{1}{G} \\sum_{i=1}^G \\frac{1}{|o_i|} \\sum_{t=1}^{|o_i|} \\min\\left(\\frac{\\pi_\\theta(o_{i,t}|q, o_{i,<t})}{\\pi_{\\theta_{old}}(o_{i,t}|q, o_{i,<t})} \\hat{A}_{i,t}, \\text{clip}\\left(\\frac{\\pi_\\theta(o_{i,t}|q, o_{i,<t})}{\\pi_{\\theta_{old}}(o_{i,t}|q, o_{i,<t})}, 1-\\varepsilon, 1+\\varepsilon\\right) \\hat{A}_{i,t}\\right) - \\beta D_{KL}(\\pi_\\theta || \\pi_{ref})$$\n",
        "\n",
        "**Variable definitions:**\n",
        "- $q$: input query/prompt (e.g., \"Solve 3+5=\")\n",
        "- $o_i$: $i$-th output/response in the group (e.g., \"8\")\n",
        "- $G$: group size (number of responses per prompt)\n",
        "- $\\pi_\\theta(o_{i,t}|q, o_{i,<t})$: probability of token $t$ in response $i$ under current policy\n",
        "- $\\pi_{\\theta_{old}}$: old policy (frozen during gradient step)\n",
        "- $\\pi_{ref}$: reference policy (frozen throughout training)\n",
        "\n",
        "**KL divergence estimator** (unbiased, Schulman 2020):\n",
        "$$D_{KL}(\\pi_\\theta || \\pi_{ref}) = \\frac{\\pi_{ref}(o_{i,t}|q, o_{i,<t})}{\\pi_\\theta(o_{i,t}|q, o_{i,<t})} - \\log\\frac{\\pi_{ref}(o_{i,t}|q, o_{i,<t})}{\\pi_\\theta(o_{i,t}|q, o_{i,<t})} - 1$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_logprobs(model, tokens, mask):\n",
        "  # Compute log probabilities for autoregressive language modeling\n",
        "  logits = model(tokens).logits # (B, T, V) - raw logits for all vocab tokens\n",
        "  log_probs = F.log_softmax(logits, dim=-1) # (B, T, V) - log π(token | context) for all tokens\n",
        "  \n",
        "  # Shift for next-token prediction: predict token t+1 from context up to t\n",
        "  log_probs = log_probs[:, :-1, :] # (B, T-1, V) - remove last position (no next token to predict)\n",
        "  shift_tokens = tokens[:, 1:] # (B, T-1) - target tokens (what we're trying to predict)\n",
        "  shift_mask = mask[:, 1:] # (B, T-1) - mask for generated tokens only\n",
        "  \n",
        "  # Extract log π(actual_token | context) for each generated token\n",
        "  tok_logps = log_probs.gather(-1, shift_tokens.unsqueeze(-1)).squeeze(-1) # (B, T-1)\n",
        "  tok_logps = tok_logps * shift_mask # Zero out prompt tokens, keep only generated tokens\n",
        "  \n",
        "  # For GRPO formula: sum over output tokens to get sequence log probability\n",
        "  # This implements: log π(o_i | q) = Σ_t log π(o_{i,t} | q, o_{i,<t})\n",
        "  seq_logps = tok_logps.sum(dim=-1) # (B) - used for PPO ratio calculation\n",
        "\n",
        "  # Return both: seq_logps for ratios as approximation, tok_logps for KL divergence (token-level)\n",
        "  return seq_logps, tok_logps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GRPO Training Step\n",
        "\n",
        "**Key GRPO algorithm:**\n",
        "1. Generate group of responses per prompt\n",
        "2. Compute advantages relative to group mean: `(reward - group_mean) / group_std`\n",
        "3. Apply PPO clipping with KL regularization\n",
        "\n",
        "This avoids needing a separate value function - the group statistics provide the baseline.\n",
        "\n",
        "**policy_steps**: Number of gradient updates per batch. If policy_steps=1, we do standard single-step updates. Multiple steps (e.g., 2) allow more aggressive optimization but risk overfitting to the current batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgE2NLMS0rcV"
      },
      "outputs": [],
      "source": [
        "def grpo_step(model, ref_model, tokenizer, batch, optimizer, group_size=6,\n",
        "              temperature=1.0, clip_eps=0.6, beta=0.02, policy_steps=2):\n",
        "  # GRPO Step 1: Generate groups of responses and compute group-relative advantages\n",
        "  samples = []\n",
        "  all_rewards = []\n",
        "  \n",
        "  for prompt, target in batch:\n",
        "    # Generate G responses for this prompt (implements: {o_i}_{i=1}^G ~ π_θ_old(O|q))\n",
        "    tokens, mask = generate_group(model, tokenizer, prompt,\n",
        "                                  group_size=group_size,\n",
        "                                  temperature=temperature)\n",
        "\n",
        "    # Compute rewards for each response in the group\n",
        "    group_rewards = []\n",
        "    for t in tokens:\n",
        "      response = tokenizer.decode(t, skip_special_tokens=True)\n",
        "      group_rewards.append(reward(response, target))\n",
        "    group_rewards = torch.tensor(group_rewards, device=device)\n",
        "    all_rewards.append(group_rewards)\n",
        "\n",
        "    # GRPO key insight: compute advantages relative to group statistics\n",
        "    # This replaces the value function baseline in standard PPO\n",
        "    r_avg = group_rewards.mean()  # Group mean reward \\bar{R}\n",
        "    r_std = group_rewards.std(unbiased=False)  # Group std σ_R\n",
        "    advantages = (group_rewards - r_avg) / (r_std + 1e-8)  # Normalized advantages \\hat{A}_t\n",
        "\n",
        "    # Store samples for batch processing\n",
        "    for i in range(len(tokens)):\n",
        "      samples.append({\n",
        "          \"a\": advantages[i].detach(), # Advantage for this response\n",
        "          \"tokens\": tokens[i],\n",
        "          \"mask\": mask[i]\n",
        "      })\n",
        "\n",
        "  # GRPO Step 2: Batch all samples for efficient processing\n",
        "  N = len(samples)  # Total number of responses across all prompts\n",
        "  max_T = max([sample[\"tokens\"].shape[0] for sample in samples])\n",
        "\n",
        "  # Pad sequences to same length for batching\n",
        "  tokens = torch.full((N, max_T), fill_value=tokenizer.pad_token_id,\n",
        "                      dtype=torch.long, device=device)\n",
        "  mask = torch.zeros((N, max_T), device=device)\n",
        "  a = torch.empty((N,), dtype=torch.float32, device=device)\n",
        "\n",
        "  for i, sample in enumerate(samples):\n",
        "    T = sample[\"tokens\"].shape[0]\n",
        "    tokens[i, :T] = sample[\"tokens\"]\n",
        "    mask[i, :T] = sample[\"mask\"]\n",
        "    a[i] = sample[\"a\"]\n",
        "\n",
        "  # GRPO Step 3: Compute baseline probabilities (frozen during optimization)\n",
        "  with torch.no_grad():\n",
        "      seq_logps_old, _ = get_logprobs(model, tokens, mask)  # π_θ_old for PPO ratio\n",
        "      _, ref_tok_logps = get_logprobs(ref_model, tokens, mask)  # π_ref for KL\n",
        "\n",
        "  # GRPO Step 4: Policy optimization loop\n",
        "  for _ in range(policy_steps):\n",
        "    # Compute current policy probabilities (with gradients)\n",
        "    seq_logps, tok_logps = get_logprobs(model, tokens, mask)\n",
        "    \n",
        "    # PPO clipped surrogate loss (using sequence-level ratios as approximation)\n",
        "    # Note: Paper uses token-level ratios, this is a common simplification\n",
        "    ratio = torch.exp(seq_logps - seq_logps_old)  # π_θ(o_i|q) / π_θ_old(o_i|q)\n",
        "    surr1 = ratio * a  # Unclipped surrogate\n",
        "    surr2 = torch.clamp(ratio, 1-clip_eps, 1+clip_eps) * a  # Clipped surrogate\n",
        "    surr = torch.minimum(surr1, surr2)  # PPO clipping: min(surr1, surr2)\n",
        "\n",
        "    # Unbiased KL divergence estimator (Schulman 2020)\n",
        "    # Operates on token-level probabilities as required by the estimator\n",
        "    kl = (ref_tok_logps - tok_logps).mean(dim=1)  # log(π_ref/π_θ) averaged over tokens\n",
        "    D_kl = torch.exp(kl) - kl - 1  # Unbiased estimator: exp(x) - x - 1\n",
        "    \n",
        "    # Final GRPO loss: -PPO_objective + β * KL_penalty. Averaged over all outputs for all inputs in a batch\n",
        "    loss = (-surr + beta * D_kl).mean()\n",
        "\n",
        "    # Standard optimization step\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
        "    optimizer.step()\n",
        "\n",
        "  return {\"mean_rewards\": (torch.cat(all_rewards)).mean().cpu()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Loop\n",
        "\n",
        "Run GRPO for 50 steps. Each step processes a batch of 8 prompts, generating 6 responses per prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yovzuB2Va8LE",
        "outputId": "e17f8224-f817-4119-8a0d-4d1eddc221bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5: reward: 0.125\n",
            "10: reward: 0.344\n",
            "15: reward: 0.365\n",
            "20: reward: 0.594\n",
            "25: reward: 0.458\n",
            "30: reward: 0.792\n",
            "35: reward: 0.667\n",
            "40: reward: 0.667\n",
            "45: reward: 0.875\n",
            "50: reward: 0.854\n"
          ]
        }
      ],
      "source": [
        "optimizer = AdamW(model.parameters(), lr=5e-6)\n",
        "\n",
        "for step in range(1, 51):\n",
        "  batch = get_batch(size=16)\n",
        "  info = grpo_step(model, ref_model, tokenizer, batch, optimizer, group_size=6,\n",
        "              temperature=0.7, clip_eps=0.2, beta=0.01)\n",
        "  if step%5 == 0:\n",
        "    print(f'{step}: reward: {info[\"mean_rewards\"]:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation\n",
        "\n",
        "Compare reference model (untrained) vs trained model performance on arithmetic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_batch = get_batch(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcYHu2ioDYQj",
        "outputId": "90783403-bfa0-4fc2-9d14-b0ae199b6471"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average accuracy = 0.14\n",
            "\n",
            "✓ Solve 2+4=6. Target: 6\n",
            "✓ Solve 4*4=16. Target: 16\n",
            "✗ Solve 6*5=20. Target: 30\n",
            "✗ Solve 3+3=7. Target: 6\n",
            "✗ Solve 3+0=10. Target: 3\n",
            "✗ Solve 4+7=10. Target: 11\n",
            "✗ Solve 6+0=10. Target: 6\n",
            "✗ Solve 0*9=10. Target: 0\n",
            "✗ Solve 4+1=6. Target: 5\n",
            "✗ Solve 1*9=10. Target: 9\n"
          ]
        }
      ],
      "source": [
        "ref_acc = eval(ref_model, eval_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAJn36D34lKK",
        "outputId": "2d80d01b-229e-4c07-eaa6-db5fea371513"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average accuracy = 0.82\n",
            "\n",
            "✓ Solve 0*2=0. Target: 0\n",
            "✓ Solve 8+9=17. Target: 17\n",
            "✗ Solve 5*3=13. Target: 15\n",
            "✓ Solve 8+2=10. Target: 10\n",
            "✓ Solve 2+1=3. Target: 3\n",
            "✓ Solve 1*4=4. Target: 4\n",
            "✗ Solve 6+6=14. Target: 12\n",
            "✓ Solve 0*6=0. Target: 0\n",
            "✓ Solve 0+9=9. Target: 9\n",
            "✓ Solve 2+7=9. Target: 9\n"
          ]
        }
      ],
      "source": [
        "trained_acc = eval(model, eval_batch)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
